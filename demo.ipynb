{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we shall learn the stochastic dynamics of the one-dimensional Fredrickson-Andersen (FA) model, consisting of binary spins on a lattice, through observation of a single trajectory. The trajectory of length $T$, obtained through kinetic (or continuous-time) Monte Carlo, consists of configurations and their associated residence times:\n",
    "\\begin{equation*}\n",
    "\\omega = \\mathcal{C}_0  \\xrightarrow[]{\\Delta t_{\\mathcal{C}_0}} \\mathcal{C}_1 \\xrightarrow[]{\\Delta t_{\\mathcal{C}_1}} \\cdots\\mathcal{C}_{K-1} \\xrightarrow[]{\\Delta t_{\\mathcal{C}_{K-1}}}\\mathcal{C}_K \\xrightarrow[]{\\Delta t_{K}} \\mathcal{C}_K\n",
    "\\end{equation*}\n",
    "where $\\Delta t_{K} = T-\\sum_{k=0}^{K-1} \\Delta t_{\\mathcal{C}_k}$. The trajectory log-likelihood is given by \n",
    "\\begin{equation*}\n",
    "U_\\omega =\\sum_{k=0}^{K-1} \\left(\\ln W_{\\mathcal{C}_k \\to \\mathcal{C}_{k+1}}-\\Delta t_{\\mathcal{C}_k} R_{\\mathcal{C}_k}\\right)  - \\Delta t_K R_{\\mathcal{C}_K}\n",
    "\\end{equation*}\n",
    "where $ W_{\\mathcal{C}_k \\to \\mathcal{C}_{k+1}}$ is the transition rate between configurations $\\mathcal{C}_k$ and $\\mathcal{C}_{k+1}$, and $R_{\\mathcal{C}_k} \\equiv \\sum_{\\mathcal{C}'} W_{\\mathcal{C}_k \\to \\mathcal{C}'}$, the sum running over all transitions allowed from $\\mathcal{C}_k$. \n",
    "\n",
    "For the FA model, the transition rate for flipping a spin depends only on the state of its nearest neighbours.  The table below contains the rate for all possible processes, where $c$ is a constant equal to the average number of up-spins. We employ periodic boundary conditions in our simulations.\n",
    "\n",
    "| Process | Rate | \n",
    "| --- | --- | \n",
    "| `000` $\\rightarrow$ `010` | 0 |\n",
    "| `010` $\\rightarrow$ `000` | 0 |\n",
    "| `001` $\\rightarrow$ `011` | $c$ |\n",
    "| `100` $\\rightarrow$ `110`| $c$ |\n",
    "| `101` $\\rightarrow$ `111`| $2c$ |\n",
    "| `110` $\\rightarrow$ `100` | $1-c$ |\n",
    "| `011` $\\rightarrow$ `001`| $1-c$ |\n",
    "| `111` $\\rightarrow$ `101`| $2(1-c)$ |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def transition_rates_and_R(config,c):\n",
    "    transition_rates = (config*(1-c) + (1-config)*c) * ((np.roll(config, 1) + np.roll(config, -1)))\n",
    "    R = np.sum(transition_rates)\n",
    "    return transition_rates, R\n",
    "\n",
    "\n",
    "\n",
    "def sample_FA_trajectory(T,L,c):\n",
    "    t = 0\n",
    "    log_likelihood = 0\n",
    "\n",
    "    #store the configurations and residence times\n",
    "    configs = np.zeros((int(T*L*c*1.5), L))\n",
    "    times = np.zeros((configs.shape[0]))\n",
    "\n",
    "    i = 0\n",
    "    config = np.random.choice([0,1], L, p= [1-c,c])\n",
    "    configs[i] = config\n",
    "\n",
    "    transition_rates, R = transition_rates_and_R(config,c)\n",
    "\n",
    "    dt = np.random.exponential(scale = 1/R, size = 1)[0]\n",
    "\n",
    "    while t + dt < T:\n",
    "\n",
    "        times[i] = dt\n",
    "        flip_id = np.random.choice(L, 1, p = transition_rates/R)[0]\n",
    "        log_likelihood += np.log(transition_rates[flip_id]) - times[i]*R\n",
    "\n",
    "        t += dt\n",
    "        i+=1\n",
    "\n",
    "        config = config.copy()\n",
    "        config[flip_id] = 1 - config[flip_id]\n",
    "        configs[i] = config\n",
    "        transition_rates, R = transition_rates_and_R(config,c)\n",
    "        dt = np.random.exponential(scale = 1/R, size = 1)[0]\n",
    "\n",
    "    times[i] = T-t\n",
    "    log_likelihood -= R*times[i]\n",
    "\n",
    "    return configs[:i+1],times[:i+1], log_likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to learn these dynamics without making assumptions such as locality of the interactions or translational invariance, we must use an efficient way of encoding all the possible transition rates (the number of which grows exponentially with system size). Here, we do so using a neural network. The neural network used to encode the dynamics is a transformer. It consists of multiple layers:\n",
    "- We first embed the particles' states and positions into a higher-dimensional space. This is done by using `torch.nn.embedding` layers. The embedding of the state and position are added together.\n",
    "- These embeddings are passed to a `torch.nn.TransformerEncoderLayer`. This layer combines multi-head attention with fully-connected networks and layer normalization acting on the output of the attention layers for each particle. This step can be repeated multiple times; the features of the previous layer are then used as input for the next one.\n",
    "- Finally, we use a fully-connected neural network acting on the output features of the transformer to calculate the log-rate $\\ln W$ for each possible transition; for the specific case of the FA model, this consists of flipping each spin. The output of the model is hence a new sequence which contains the log-rate for flipping each spin of the original sequence.\n",
    "\n",
    "Through the attention layers, the transformer needs to decide which spins affect the transition rate for flipping a certain spin; we do not provide any prior information about locality of interaction, translation invariance, or the boundary conditions. If such prior physical information would be available, it can easily be incorporated into the model (e.g. by restricting the range of the attention).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "dtype = torch.float32\n",
    "\n",
    "class TransformerModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, seq_length, embedding_dim, num_heads, num_layers, dim_feedforward):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        #learnable embeddings for the state and position of each spin\n",
    "        self.spin_emb = torch.nn.Embedding(input_dim, embedding_dim)\n",
    "        self.pos_emb = torch.nn.Embedding(seq_length, embedding_dim)\n",
    "\n",
    "        #transformer layers model the interactions between the spins for the rate calculations\n",
    "        encoder_layers = torch.nn.TransformerEncoderLayer(embedding_dim, num_heads, dim_feedforward, dropout=0, batch_first = True)\n",
    "        self.transformer = torch.nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "\n",
    "        #the decoder calculates ln W for a certain transition by processing the output features of the transformer. Here, we can do this with just a single-layer fully-connected net.\n",
    "        #for the FA model, we only need one output node (rate for flipping a spin), for other models with more possible particle updates this should be increased\n",
    "        model = []\n",
    "        model += [torch.nn.Linear(embedding_dim, 1)]\n",
    "        self.decoder = torch.nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        #embed the particles' state and position\n",
    "        spin_emb = self.spin_emb(x)\n",
    "        b,t,k = spin_emb.size()\n",
    "        positions = torch.arange(self.seq_length, device=device)\n",
    "        pos_emb = self.pos_emb(positions)[None, :,:].expand(b,t,k)\n",
    "\n",
    "        input = spin_emb + pos_emb\n",
    "\n",
    "        #calculate the transformer features\n",
    "        output = self.transformer(input)\n",
    "\n",
    "        #calculate the transition rates\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When provided with a trajectory consisting of configurations and residence times, we can optimize the transformer to learn the dynamics which generated this trajectory. We do so by minimizing the negative log-likelihood of the trajectory as calculated by the transformer through gradient descent. As transformers use a large amount of memory, we do this calculation in smaller batches. We then have two options for updating the transformer's weights:\n",
    "- We update the weights after processing each batch in order to maximize the log-likelihood of the shorter trajectory contained in this batch.\n",
    "- We store the gradients corresponding to each batch, and only update the weights once the full trajectory is processed.\n",
    "\n",
    "The second option naturally leads to more accurate gradients to maximize the log-likelihood of the provided trajectory, but requires extensive computation for each weight update. We typically start by using the first option, where we aggregate the gradients of only a few batches before each weight update, to quickly get near the maximum trajectory likelihood. The second option is then used for further fine-tuning of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DynamicsLearner(torch.nn.Module):\n",
    "    def __init__(self,configs,times, exact, T, input_dim, embedding_dim, num_heads, num_layers, dim_feedforward):\n",
    "        super(DynamicsLearner, self).__init__()\n",
    "\n",
    "        self.L = configs[0].shape[0]\n",
    "\n",
    "        self.transformer_logrates = TransformerModel(input_dim, self.L, embedding_dim, num_heads, num_layers, dim_feedforward)\n",
    "        self.transformer_logrates.to(dtype).to(device)\n",
    "\n",
    "        self.times = torch.from_numpy(times).to(dtype)\n",
    "        self.configs = torch.from_numpy(configs).to(torch.int64)\n",
    "        self.exact = exact\n",
    "        self.T = T\n",
    "\n",
    "    #calculate the possible transition log-rates and escape rate for a batch of configurations\n",
    "    def log_transition_rates_and_R_batched(self, configs):\n",
    "        configs = configs.to(device)\n",
    "        log_rates = self.transformer_logrates(configs)[...,0]\n",
    "        R = torch.sum(torch.exp(log_rates), dim =1)\n",
    "        return log_rates, R\n",
    "\n",
    "\n",
    "    #calculate log-likelihood of part of the trajectory; \\sum_k ln W_{C_k->C+{k+1}} - R \\Delta t_{C_k}\n",
    "    def log_likelihood_trajectory_batched(self, start, end, calc_last = False):\n",
    "        log_transition_rates, R = self.log_transition_rates_and_R_batched(self.configs[start:end])\n",
    "        if not calc_last:\n",
    "            #check which spin has flipped for each transition\n",
    "            ids_flipped = torch.where(self.configs[start:end] - self.configs[start+1:end + 1])\n",
    "            \n",
    "            log_likelihood = log_transition_rates[ids_flipped].sum() - (self.times[start:end].to(device)*R).sum()\n",
    "        else:\n",
    "            ids_flipped = torch.where(self.configs[start:end-1] - self.configs[start+1:end])\n",
    "            log_likelihood = log_transition_rates[:-1][ids_flipped].sum() - (self.times[start:end-1].to(device)*R[:-1]).sum()\n",
    "\n",
    "            #R_K \\Delta t_{C_K} for last configurations\n",
    "            log_likelihood -= R[-1]*self.times[-1].to(device)\n",
    "        return log_likelihood\n",
    "\n",
    "\n",
    "\n",
    "    def train_batched(self, n_iter, lr, batch_size,update_batch, batch_per_update, grad_clip):\n",
    "        #use the Adam optimizer to train the net\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr)\n",
    "\n",
    "        losses = np.zeros(n_iter)\n",
    "        \n",
    "        steps = self.times.shape[0] // batch_size + 1  if batch_size < self.times.shape[0] else 1\n",
    "\n",
    "        for i in range(n_iter):\n",
    "\n",
    "            running_loss = 0\n",
    "\n",
    "            for j in tqdm(range(steps)):\n",
    "\n",
    "                #calculate the loss on each batch (i.e. part of the trajectory)\n",
    "                #multiply loss by -1 as we want to minimize the negative log-likelihood\n",
    "                if j < steps - 1:\n",
    "                    loss = -1* (self.log_likelihood_trajectory_batched(j*batch_size, (j+1)*batch_size))\n",
    "                else:\n",
    "                    loss = -1 *(self.log_likelihood_trajectory_batched(j*batch_size, self.times.shape[0], calc_last = True))\n",
    "\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                if update_batch and j % batch_per_update == 0:\n",
    "                    #update weights on smaller parts of the trajectory, and reset gradients\n",
    "\n",
    "                    if grad_clip != 0:\n",
    "                        torch.nn.utils.clip_grad_norm_(self.parameters(), grad_clip)\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "                running_loss += loss.detach()\n",
    "\n",
    "            \n",
    "            if not update_batch:\n",
    "                #update weights based on log-likelihood of the full trajectory\n",
    "                if grad_clip != 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.parameters(), grad_clip)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            tot_loss = running_loss.cpu().numpy()\n",
    "\n",
    "            losses[i] = tot_loss\n",
    "\n",
    "            print(\"Step: \"+ str(i+1)+\"\\t Exact: \" + str(self.exact/self.T) + \"\\t\"+\"Transformer: \" + str(-1*tot_loss/self.T))\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:05<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1\t Exact: -3.9643263084372964\tTransformer: -6.4935328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:05<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2\t Exact: -3.9643263084372964\tTransformer: -5.248736328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:04<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3\t Exact: -3.9643263084372964\tTransformer: -4.593200390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:04<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4\t Exact: -3.9643263084372964\tTransformer: -4.117807421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:05<00:00,  5.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5\t Exact: -3.9643263084372964\tTransformer: -4.029580859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:05<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6\t Exact: -3.9643263084372964\tTransformer: -4.0115578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:04<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 7\t Exact: -3.9643263084372964\tTransformer: -4.002492578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:04<00:00,  5.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 8\t Exact: -3.9643263084372964\tTransformer: -3.994111328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:04<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 9\t Exact: -3.9643263084372964\tTransformer: -3.98926171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:04<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10\t Exact: -3.9643263084372964\tTransformer: -3.986030859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:04<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1\t Exact: -3.9643263084372964\tTransformer: -3.97937265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:04<00:00,  5.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2\t Exact: -3.9643263084372964\tTransformer: -3.9729421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:04<00:00,  5.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3\t Exact: -3.9643263084372964\tTransformer: -3.97064296875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:04<00:00,  5.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4\t Exact: -3.9643263084372964\tTransformer: -3.968324609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:05<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5\t Exact: -3.9643263084372964\tTransformer: -3.967577734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:04<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6\t Exact: -3.9643263084372964\tTransformer: -3.96716328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:04<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 7\t Exact: -3.9643263084372964\tTransformer: -3.966657421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:05<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 8\t Exact: -3.9643263084372964\tTransformer: -3.966657421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:04<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 9\t Exact: -3.9643263084372964\tTransformer: -3.96661015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:05<00:00,  5.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10\t Exact: -3.9643263084372964\tTransformer: -3.966287890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#generate a trajectory\n",
    "\n",
    "T = 1e4\n",
    "L = 10\n",
    "c = 0.3\n",
    "\n",
    "configs,times, exact = sample_FA_trajectory(T, L, c)\n",
    "\n",
    "\n",
    "#define the model parameters\n",
    "\n",
    "input_dim =2 #number of spin states (here = 2 for binary spins)\n",
    "embedding_dim = 64 \n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "dim_feedforward = int(4*embedding_dim)\n",
    "\n",
    "dynamics = DynamicsLearner(configs,times, exact, T, input_dim, embedding_dim, num_heads, num_layers, dim_feedforward)\n",
    "\n",
    "#first do optimization on batches of the full trajectory\n",
    "num_iter = 10\n",
    "lr = 5e-4\n",
    "batch_size = int(1e3)\n",
    "update_batch = True\n",
    "batch_per_update = 1\n",
    "grad_clip = 100 \n",
    "losses_batched = dynamics.train_batched(num_iter,lr , batch_size,update_batch, batch_per_update, grad_clip)\n",
    "\n",
    "#then fine-tune weights on full trajectory\n",
    "num_iter = 10\n",
    "lr = 1e-4\n",
    "losses_full = dynamics.train_batched(num_iter,lr , batch_size,False, 0, grad_clip)\n",
    "\n",
    "losses = np.concatenate((losses_batched, losses_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training is finished, we can evaluate the trajectory log-likelihood for both the exact dynamics and those learned with the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.axhline(exact/T, ls = \"-\", color=\"k\", label = \"Exact\")\n",
    "plt.plot(np.arange(losses.shape[0])+1,-1*losses/T, label = \"Transformer\")\n",
    "plt.xlim([1,losses.shape[0]])\n",
    "plt.xlabel(\"#epochs\", fontsize = 16)\n",
    "plt.ylabel(r\"$U_\\omega^\\theta/T$\", fontsize = 16)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57d879c1bab31ddce3f98747a90aac1ecdf0d747d4f9b6f921c92f41b19b21c9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('pytorch_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
